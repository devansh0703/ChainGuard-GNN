import streamlit as st
import requests
import os
import subprocess
import time
import networkx as nx
import matplotlib.pyplot as plt
import numpy as np
import torch
import plotly.graph_objects as go

st.set_page_config(page_title="ChainGuard Dashboard", page_icon="ðŸ”", layout="wide")

st.title("ðŸ” ChainGuard: DeFi Fraud Detection Dashboard")

st.markdown("""
This dashboard provides real-time fraud detection insights for DeFi transactions using advanced Graph Neural Networks.
Monitor risk scores, view alerts, and analyze transaction patterns.
""")

# API base URL
API_BASE = "http://localhost:8000"

# Function to load data
@st.cache_data
def load_data():
    try:
        response = requests.get(f"{API_BASE}/stats")
        if response.status_code == 200:
            return response.json()
        else:
            return None
    except:
        return None

@st.cache_data
def load_top10():
    try:
        response = requests.get(f"{API_BASE}/top10")
        if response.status_code == 200:
            return response.json()["top10"]
        else:
            return []
    except:
        return []

# Sidebar for navigation
st.sidebar.header("Navigation")
page = st.sidebar.radio("Go to", ["Overview", "Top Risky Transactions", "Transaction Lookup", "Analytics", "Plots", "Train Model", "Graph Visualization"])

if page == "Train Model":
    st.header("ðŸš€ Train Model")
    st.markdown("Retrain the GIN ensemble model with custom epochs. This will update all analytics and plots.")
    
    epochs = st.number_input("Number of Epochs per Model", min_value=10, max_value=200, value=100, step=10)
    
    if st.button("Start Training"):
        with st.spinner("Training model... This may take several minutes."):
            try:
                # Run the training script
                result = subprocess.run([
                    "python3", "train_gin.py", "--epochs", str(epochs)
                ], capture_output=True, text=True, cwd="/home/devansh/Downloads/archive", env={**os.environ, "PYTORCH_CUDA_ALLOC_CONF": "expandable_segments:True"})
                
                if result.returncode == 0:
                    st.success("Training completed successfully!")
                    st.text("Training Output:")
                    st.code(result.stdout[-2000:], language="text")  # Show last 2000 chars
                    
                    # Clear cache to reload data
                    load_data.clear()
                    load_top10.clear()
                    
                    st.rerun()
                else:
                    st.error("Training failed!")
                    st.text("Error Output:")
                    st.code(result.stderr, language="text")
            except Exception as e:
                st.error(f"Error running training: {e}")

if page == "Overview":
    st.header("ðŸ“Š Overview")
    stats = load_data()
    if stats:
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Total Transactions", stats["total_transactions"])
        with col2:
            st.metric("High Risk (>80)", stats["high_risk"])
        with col3:
            st.metric("Medium Risk (50-80)", stats["medium_risk"])
        with col4:
            st.metric("Low Risk (â‰¤50)", stats["low_risk"])
    else:
        st.error("Failed to load stats from API. Make sure the API is running.")

elif page == "Top Risky Transactions":
    st.header("ðŸš¨ Top 10 Riskiest Transactions")
    top10 = load_top10()
    if top10:
        for i, item in enumerate(top10, 1):
            with st.expander(f"#{i} Transaction Index: {item['index']} - Risk Score: {item['risk_score']}"):
                st.write(f"**Alert:** {item['alert']}")
                st.write(f"**Confidence:** {item['confidence']}")
                st.write(f"**Predicted Label:** {'Suspicious' if item['predicted_label'] == 1 else 'Licit'}")
                if item['risk_score'] > 80:
                    st.error("High Risk - Immediate Action Required")
                elif item['risk_score'] > 50:
                    st.warning("Medium Risk - Monitor Closely")
                else:
                    st.success("Low Risk")
    else:
        st.error("Failed to load top 10 from API. Make sure the API is running.")

elif page == "Transaction Lookup":
    st.header("ðŸ”Ž Transaction Risk Lookup")
    index = st.number_input("Enter Transaction Index", min_value=0, value=0, step=1)
    if st.button("Lookup Risk"):
        try:
            response = requests.get(f"{API_BASE}/risk/{index}")
            if response.status_code == 200:
                data = response.json()
                if "error" in data:
                    st.error(data["error"])
                else:
                    col1, col2 = st.columns(2)
                    with col1:
                        st.metric("Risk Score", data["risk_score"])
                        st.write(f"**Alert:** {data['alert']}")
                    with col2:
                        st.write(f"**Confidence:** {data['confidence']}")
                        st.write(f"**Predicted Label:** {'Suspicious' if data['predicted_label'] == 1 else 'Licit'}")
                    
                    # Risk gauge
                    if data["risk_score"] > 80:
                        st.error("ðŸš¨ High Risk - Flag for Investigation")
                    elif data["risk_score"] > 50:
                        st.warning("âš ï¸ Medium Risk - Add to Watchlist")
                    else:
                        st.success("âœ… Low Risk - Normal Transaction")
            else:
                st.error("Failed to fetch risk data from API")
        except Exception as e:
            st.error(f"API connection error: {e}")

elif page == "Analytics":
    st.header("ðŸ“ˆ Analytics")
    st.markdown("Model Performance Metrics (from latest training):")
    st.write("- **F1 Score:** 0.91 (Suspicious Class)")
    st.write("- **Precision:** 0.91")
    st.write("- **Recall:** 0.90")
    st.write("- **AP (Average Precision):** 0.9559")
    st.write("- **AUC:** 0.9852")
    st.markdown("**Threshold Tuning:** Cost-sensitive threshold at 0.59 for balanced FP/FN costs.")
    st.markdown("*Note: Metrics update after retraining the model.*")

elif page == "Plots":
    st.header("ðŸ“Š Visualizations")
    plots_dir = "plots"
    if os.path.exists(plots_dir):
        plot_files = [f for f in os.listdir(plots_dir) if f.endswith(".png")]
        if plot_files:
            selected_plot = st.selectbox("Select Plot", plot_files)
            st.image(os.path.join(plots_dir, selected_plot), caption=selected_plot,width=900)
        else:
            st.write("No plots available. Train the model to generate plots.")
    else:
        st.write("Plots directory not found.")

elif page == "Graph Visualization":
    st.header("ðŸ“Š Graph Visualization")
    st.markdown("Interactive visualization of a connected component from the transaction graph with risk-based coloring. Zoom and pan to explore!")

    # Load data
    @st.cache_data
    def load_graph_data():
        # Load graph
        data = torch.load('graph_data.pt', weights_only=False)
        G = nx.Graph()
        G.add_edges_from(data.edge_index.t().tolist())
        
        # Load risks
        import pickle
        with open('all_risks.pkl', 'rb') as f:
            all_risks = pickle.load(f)
        risk_dict = {idx: risk for idx, risk, _, _, _ in all_risks}
        
        return G, risk_dict

    G, risk_dict = load_graph_data()

    # Find connected components
    components = list(nx.connected_components(G))
    component_sizes = sorted([len(c) for c in components], reverse=True)
    
    # Select the top 10 largest components
    components_sorted = sorted(components, key=len, reverse=True)
    top_components = components_sorted[:10]
    
    for i, selected_component in enumerate(top_components):
        if len(selected_component) > 5000:
            continue  # Skip if too large
        subgraph = G.subgraph(selected_component)
        
        # Get positions
        pos = nx.spring_layout(subgraph, seed=42)
        
        # Prepare node data
        node_x = []
        node_y = []
        node_colors = []
        node_texts = []
        for node in subgraph.nodes():
            x, y = pos[node]
            node_x.append(x)
            node_y.append(y)
            risk = risk_dict.get(node, 0)
            if risk > 80:
                node_colors.append('red')
            elif risk > 50:
                node_colors.append('orange')
            else:
                node_colors.append('green')
            node_texts.append(f'Node {node}<br>Risk: {risk:.2f}')
        
        # Prepare edge data
        edge_x = []
        edge_y = []
        for edge in subgraph.edges():
            x0, y0 = pos[edge[0]]
            x1, y1 = pos[edge[1]]
            edge_x.extend([x0, x1, None])
            edge_y.extend([y0, y1, None])
        
        # Create figure
        fig = go.Figure()
        
        # Add edges
        fig.add_trace(go.Scatter(
            x=edge_x, y=edge_y,
            line=dict(width=0.5, color='gray'),
            hoverinfo='none',
            mode='lines',
            name='Edges'
        ))
        
        # Add nodes
        fig.add_trace(go.Scatter(
            x=node_x, y=node_y,
            mode='markers',
            hoverinfo='text',
            text=node_texts,
            marker=dict(
                color=node_colors,
                size=10,
                line_width=2
            ),
            name='Nodes'
        ))
        
        fig.update_layout(
            title=f"Largest Component #{i+1} ({len(selected_component)} nodes) - Zoom & Pan to Explore",
            title_x=0.5,
            showlegend=False,
            hovermode='closest',
            margin=dict(b=20, l=5, r=5, t=40),
            xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
            yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)
        )
        
        st.plotly_chart(fig, use_container_width=True)

    st.markdown("---")
    st.subheader("ðŸ” Node Subgraph Lookup")
    node_index = st.number_input("Enter Node Index", min_value=0, max_value=len(G)-1, value=0)
    
    if st.button("Show Subgraph"):
        if node_index in G:
            # Ego graph with radius 2
            ego = nx.ego_graph(G, node_index, radius=2)
            
            if len(ego) > 50:
                st.warning("Subgraph too large to display. Try a different node or smaller radius.")
            else:
                # Colors for ego
                ego_colors = []
                for n in ego.nodes():
                    r = risk_dict.get(n, 0)
                    if r > 80:
                        ego_colors.append('red')
                    elif r > 50:
                        ego_colors.append('orange')
                    else:
                        ego_colors.append('green')
                
                fig2, ax2 = plt.subplots(figsize=(8, 6))
                pos2 = nx.spring_layout(ego, seed=42)
                nx.draw(ego, pos2, node_color=ego_colors, node_size=100, edge_color='blue', alpha=0.7, ax=ax2, with_labels=True, font_size=8)
                ax2.set_title(f"Subgraph for Node {node_index} (Risk: {risk_dict.get(node_index, 0):.2f})")
                st.pyplot(fig2)
                
                # Node details
                response = requests.get(f"{API_BASE}/risk/{node_index}")
                if response.status_code == 200:
                    data = response.json()
                    st.write(f"**Risk Score:** {data['risk_score']:.2f}")
                    st.write(f"**Alert:** {data['alert']}")
                    st.write(f"**Predicted Label:** {'Suspicious' if data['predicted_label'] == 1 else 'Licit'}")
        else:
            st.error("Node not found in graph.")